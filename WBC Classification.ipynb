{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 29380,
          "sourceType": "datasetVersion",
          "datasetId": 9232
        }
      ],
      "dockerImageVersionId": 30665,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content'\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'blood-cells:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F9232%2F29380%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240831%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240831T122623Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D195d0457e17e161835b1723ef8acac2821d26fdfe2462085ee43276c87b34a7d10c448f6968a0528caf002a53f490cc390c514014e4172229de4678ba23f1ea0d848b4374d24bf1847fd0c8d3c94fb9f207f1fb8a4c1a43858387cde182404e5c7cb1976f508e68c19ab8b221d928e649ee25fcfeb6cf94b1add99e3105d9c61f5f5c290249690a4f4b8cb4d423271f40639eaef84618bd9868aa200ff341c3b6749de0942070c5f87fd7b515fd7d18d9fe04dbe3e0cb56e394b9cabe0794940d55e76e09b6d0478abb69913846515acde53dfee748f1322bba2c4fa654c470319a4124a108af0ee71618bcf4bf2af91fc44ad7a87fd258c2c55d59125e0f19f'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Cz1CSe42aBI2"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, InputLayer, BatchNormalization,UpSampling2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.ndimage import median_filter"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T06:54:27.032668Z",
          "iopub.execute_input": "2024-03-05T06:54:27.033577Z",
          "iopub.status.idle": "2024-03-05T06:54:27.039892Z",
          "shell.execute_reply.started": "2024-03-05T06:54:27.033545Z",
          "shell.execute_reply": "2024-03-05T06:54:27.038863Z"
        },
        "trusted": true,
        "id": "VUxMqHB7aBI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T06:54:27.041739Z",
          "iopub.execute_input": "2024-03-05T06:54:27.042384Z",
          "iopub.status.idle": "2024-03-05T06:54:39.64309Z",
          "shell.execute_reply.started": "2024-03-05T06:54:27.042346Z",
          "shell.execute_reply": "2024-03-05T06:54:39.641808Z"
        },
        "trusted": true,
        "id": "ama3tsdWaBI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "img_size = 128\n",
        "batch_size = 64\n",
        "\n",
        "\"\"\"train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=lambda x: median_filter(x,size=(2, 2), mode='constant', cval=0),\n",
        "    rescale = 1/255.#, brightness_range = [0.5,1.5],rotation_range=20,shear_range=10, zoom_range = 0.2,\n",
        "    #width_shift_range = 0.15, height_shift_range = 0.15, horizontal_flip = True\n",
        ")\"\"\"\n",
        "train_datagen = ImageDataGenerator(\n",
        "    #reprocessing_function=lambda x: median_filter(x, size=(2, 2, 3), mode='constant', cval=0),\n",
        "    rescale = 1./255,brightness_range = [0.5,1.5],rotation_range=0.2,shear_range=0.3, zoom_range = 0.2,\n",
        "    width_shift_range = 0.2, height_shift_range = 0.2,\n",
        "    horizontal_flip = True\n",
        ")\n",
        "\"\"\"train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=lambda x: median_filter(x,size=(2, 2), mode='constant', cval=0),\n",
        "    rescale = 1./255,\n",
        "    horizontal_flip = True)\"\"\"\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255.)\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory('/kaggle/input/blood-cells/dataset2-master/dataset2-master/images/TRAIN',\n",
        "                                                   target_size = (img_size, img_size),\n",
        "                                                   batch_size = batch_size,\n",
        "                                                   shuffle=True,\n",
        "                                                   class_mode='categorical')\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory('/kaggle/input/blood-cells/dataset2-master/dataset2-master/images/TEST_SIMPLE',\n",
        "                                                   target_size = (img_size, img_size),\n",
        "                                                   batch_size = batch_size,\n",
        "                                                   shuffle=False,\n",
        "                                                   class_mode='categorical')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory('/kaggle/input/blood-cells/dataset2-master/dataset2-master/images/TEST',\n",
        "                                                   target_size = (img_size, img_size),\n",
        "                                                   batch_size = batch_size,\n",
        "                                                   shuffle=False,\n",
        "                                                   class_mode = 'categorical')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T06:54:39.644653Z",
          "iopub.execute_input": "2024-03-05T06:54:39.644973Z",
          "iopub.status.idle": "2024-03-05T06:54:41.953321Z",
          "shell.execute_reply.started": "2024-03-05T06:54:39.644944Z",
          "shell.execute_reply": "2024-03-05T06:54:41.952285Z"
        },
        "trusted": true,
        "id": "XN3gUM49aBI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#augumented images used for training\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "augmented_images, _ = next(train_generator)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(augmented_images[i])\n",
        "    plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-03-05T06:54:41.955634Z",
          "iopub.execute_input": "2024-03-05T06:54:41.95593Z"
        },
        "trusted": true,
        "id": "cwoR72wQaBI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_layer = Input(shape=(128, 128, 3))\n",
        "\n",
        "x = Conv2D(6,\n",
        "           kernel_size=(1,1),\n",
        "           strides=(1,1),\n",
        "           padding='same',\n",
        "           activation='relu')(input_layer)\n",
        "\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "x_image_1 = UpSampling2D(size=(2, 2))(x)\n",
        "\n",
        "x = Conv2D(16, kernel_size=(5,5), strides=(1,1),padding='same',activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "x_image_2 = UpSampling2D(size=(4, 4))(x)\n",
        "\n",
        "x = Conv2D(64, kernel_size=(5,5), strides=(1,1),padding='same',activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "x_image_3 = UpSampling2D(size=(8, 8))(x)\n",
        "\n",
        "x = Conv2D(128, kernel_size=(5,5), strides=(1,1),padding='same',activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "\n",
        "x_image_4 = UpSampling2D(size=(16, 16))(x)\n",
        "\n",
        "x = Conv2D(512, kernel_size=(4,4), strides=(1,1),padding='same',activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU(alpha=0.3)(x)\n",
        "x = Dropout(0.2)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "output_layer = Dense(4, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=input_layer,outputs=output_layer)\n",
        "\n",
        "layer_outputs = [x_image_1, x_image_2, x_image_3, x_image_4]\n",
        "image_output_model = Model(inputs=input_layer, outputs=layer_outputs)\n",
        "\n",
        "example_input = np.random.random((1, 128, 128, 3))\n",
        "\n",
        "# Get the outputs of each layer for the example input\n",
        "outputs = image_output_model.predict(example_input)\n",
        "\n",
        "# Plot and display the images\n",
        "for i, output in enumerate(outputs):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    num_channels = output.shape[-1]\n",
        "    for j in range(num_channels):\n",
        "        plt.subplot(1, num_channels, j + 1)\n",
        "        plt.imshow(output[0, :, :, j])\n",
        "        plt.axis('off')\n",
        "    plt.suptitle(f\"Feature Maps of Layer {i+1}\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "dERYf2KEaBI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "trusted": true,
        "id": "NW8tYJOyaBI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator.class_indices"
      ],
      "metadata": {
        "trusted": true,
        "id": "xFAHZWWhaBI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#decayed_lr = tf.train.exponential_decay(learning_rate,\n",
        "                                        #global_step, 10000,\n",
        "                                        #0.95, staircase=True)\n",
        "#opt = tf.train.AdamOptimizer(decayed_lr, epsilon=adam_epsilon)\n",
        "\"\"\"model.compile(\n",
        "    loss = 'categorical_crossentropy',\n",
        "    optimizer = Adam(lr = 0.001,weight_decay=0.10),\n",
        "    metrics = [\"accuracy\"]\n",
        ")\"\"\"\n",
        "initial_learning_rate = 0.001\n",
        "decay_steps = 1000\n",
        "decay_rate = 0.9\n",
        "grad_decay = 0.1\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=initial_learning_rate,\n",
        "        decay_steps=decay_steps,\n",
        "        decay_rate=decay_rate\n",
        "    ),\n",
        "    beta_1=1.0 - grad_decay  # Set beta_1 to 0.9 for 0.1 gradient decay\n",
        ")\n",
        "model.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "S44o6q96aBI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "#model = load_model(\"/kaggle/input/resnet-50/tensorflow2/classification/1\")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data = val_generator,\n",
        "    epochs = 20\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "DfWFSWYraBI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "train_acc = history_dict['loss']\n",
        "val_acc = history_dict['val_loss']\n",
        "epochs = range(1, len(history_dict['loss'])+1)\n",
        "plt.plot(epochs, train_acc,'b', label='Training error')\n",
        "plt.plot(epochs, val_acc,'b', color=\"orange\", label='Validation error')\n",
        "plt.title('Training and Validation error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Error')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "N8Wu_sXTaBI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "train_acc = history_dict['accuracy']\n",
        "val_acc = history_dict['val_accuracy']\n",
        "epochs = range(1, len(history_dict['accuracy'])+1)\n",
        "plt.plot(epochs, train_acc,'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc,'b', color=\"orange\", label='Validation accuracy')\n",
        "plt.title('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "vF3aR3-GaBI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = model.predict(test_generator)"
      ],
      "metadata": {
        "trusted": true,
        "id": "eFaoAa9VaBI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\"\"\"x, y = test_generator.next()\n",
        "y_true = y\n",
        "for i in range(2487//64):\n",
        "    x, y = next(test_generator)\n",
        "    y_true = np.concatenate([y_true, y], axis = 0)\n",
        "\n",
        "print(y_true)\"\"\"\n",
        "\"\"\"x, y = test_generator.next()\n",
        "y_true = y\n",
        "for i in range(2487//64):\n",
        "    x, y = test_generator.next()\n",
        "    y_true = np.concatenate([y_true, y], axis=0)\n",
        "\n",
        "print(y_true)\"\"\"\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory('/kaggle/input/blood-cells/dataset2-master/dataset2-master/images/TEST',\n",
        "                                                   target_size=(img_size, img_size),\n",
        "                                                   batch_size=batch_size,\n",
        "                                                   shuffle=False,\n",
        "                                                   class_mode='categorical')\n",
        "\n",
        "y_true = []\n",
        "for i in range(2487 // batch_size):\n",
        "    x, y = next(test_generator)\n",
        "    y_true.append(y)\n",
        "\n",
        "y_true = np.concatenate(y_true, axis=0)\n",
        "print(y_true)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Zm_qctvDaBI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.argmax(y_true, axis = 1)\n",
        "y_preds = np.argmax(y_preds, axis = 1)\n",
        "\n",
        "print(y_true, y_preds)"
      ],
      "metadata": {
        "trusted": true,
        "id": "VM2bIBObaBI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "y_preds = y_preds[:len(y_true)]\n",
        "cm = confusion_matrix(y_true, y_preds)\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt='g', ax=ax)\n",
        "\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL'])\n",
        "ax.yaxis.set_ticklabels(['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL'])"
      ],
      "metadata": {
        "trusted": true,
        "id": "aAkvc-A5aBI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_true, y_preds, target_names = ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']))"
      ],
      "metadata": {
        "trusted": true,
        "id": "9yCQIrZpaBI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"./model.h5\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "3dFKk0RdaBI9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}